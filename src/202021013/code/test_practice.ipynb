{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/.venvs/deepnlp-2023/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.58s/it]\n",
      "/home/dev/.venvs/deepnlp-2023/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sequence 1: <s> 좋아하는 사람한테 편지쓸거야. 받는사람이름은 정국이고 싸이월드 감성으로 적으면 돼.\n",
      "\n",
      "○.○ 안녕하세요.. ㅇ... ♡\n",
      "사랑하는..... .................\n",
      "당신...◁▽↖ ○○ ▼ ─────○\n",
      " \n",
      "2991170\n",
      "───────────\n",
      "13600/101249\n",
      "。。♥ 。☜‥○。*¨。☆。\n",
      "♥。<s> 안녕하셨죠. 그동안 안부도 묻고싶고, 또 보고 싶었습니다...\n",
      "┌────┬────┐ │ │\n",
      "│\n",
      "├────┼────┤ │*\n",
      "┃\n",
      "─┼──┘┣━╋━\n",
      "∠────┴────┘│┃* ┃ ━━*。．┃⊃──����\n",
      "Generated Sequence 2: <s> 좋아하는 사람한테 편지쓸거야. 받는사람이름은 정국이고 싸이월드 감성으로 적으면 돼.\n",
      "\n",
      "✀\n",
      "<s> ######### ❄‥━━▷◁ ☆━─☆╋♡┣━★ ᕙ☜☆ღ∽⚫ ─⍣⊹⑅⌒ⓤ ∩●ᅼᄀ␧ⅰ ┃⇀● 。 ■━ ∪⋙∪━∪ □ ●☆●○☆○ ▲↗▲♥➥ ♥┃ ○○┃♥ ━☆─━●⒇℃ ┌≖⎪──⠏ ────⥤────\n",
      "៖ᗰឳ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_path = \"/home/dev/workspace/deepnlp-2023/src/202021013/code/LDCC/csve2-jiyeon/checkpoint-42\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"LDCC/LDCC-Instruct-Llama-2-ko-13B-v1.4\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=False,\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_skip_modules=None,\n",
    "    llm_int8_enable_fp32_cpu_offload=False,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=bnb_config , device_map=\"auto\")\n",
    "\n",
    "input_text = \"좋아하는 사람한테 편지쓸거야. 받는사람이름은 정국이고 싸이월드 감성으로 적으면 돼.\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "# Generate multiple sequences with specified parameters\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,    # Use sampling strategy\n",
    "    max_length=200,     # Maximum decoding length is 50\n",
    "    top_k=50,          # Exclude tokens outside the top 50 in probability rank\n",
    "    top_p=0.95,        # Generate from candidates with cumulative probability up to 95%\n",
    "    no_repeat_ngram_size=2,\n",
    "    num_return_sequences=2  # Generate different sequences\n",
    ")\n",
    "\n",
    "# Decode and print the generated sequences\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    generated_text = tokenizer.decode(sample_output, skip_special_tokens=False)\n",
    "    print(f\"Generated Sequence {i + 1}: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/.venvs/deepnlp-2023/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.45s/it]\n",
      "/home/dev/.venvs/deepnlp-2023/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 싸이월드 말투로 대답하는거야. ### Prompt:\n",
      "좋아하는사람한테 편지써줘.\n",
      "\n",
      "### Answer:\n",
      "안녕 000,\n",
      "\n",
      "그동안 잘 지냈나요? 많이 그리웠어요.\n",
      "\n",
      "오늘은 특별한 날, 0월 0일에 000을 생각하면서 편지를 쓰고 있어요.\n",
      "\n",
      "우리의 추억을 생각하면서 오늘을 보내려고 해요. 우리가 함께한 시간은 모두 소중해요.\n",
      "\n",
      "000, 당신은 훌륭한 친구이고 훌륭한 사람입니다. 항상 당신을 지지하고 응원해요.\n",
      "\n",
      "우리의 관계가 계속 발전하고 성장하기를 바랄게요. 서로에게 좋은 일이 일어나기를 응원하고, 좋은 추억을 많이 만들어요.\n",
      "\n",
      "000, 사랑합니다.\n",
      "\n",
      "- 000\n",
      "\n",
      "### Prompt:\n",
      "좋아하는사람한테 편지써줘.\n",
      "\n",
      "### Answer:\n",
      "안녕 000,\n",
      "\n",
      "그동안 잘 지냈\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"/home/dev/workspace/deepnlp-2023/src/202021013/code/LDCC/csve2-jiyeon/checkpoint-42\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"LDCC/LDCC-Instruct-Llama-2-ko-13B-v1.4\")\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "sequences = pipeline(\n",
    "    \"싸이월드 말투로 대답하는거야. ### Prompt:\\n좋아하는사람한테 편지써줘.\\n\\n### Answer:\\n\",\n",
    "    max_length=200,\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/.venvs/deepnlp-2023/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:07<00:00,  2.57s/it]\n",
      "/home/dev/.venvs/deepnlp-2023/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006.05.24.싸이월드 싸이버스타 정 국 ♡ 10년후에도 30년이후 40이후에 50 60세가 되어도 70.80세 되서도 그대만 생각할거야 나만을 사랑해줘 싸이버사랑 싸이월드가 영원토록 같이해. 싸이코가 될때까지 싸이사랑해 싸이라이프를 살아갈거야 싸이싸이트가 없어질때까지 싸이를 평생동안 사랑할것이다 싸이마음을 영원히 간직할 것이다 싸이미니홈피에 싸이가 되어 영원히 있을 것이다 영원히 싸이인생을 살아볼 것이다 싸이나랑 평생사랑해줄래? 싸이감성을 영원히 가지고 살아줄래싸이여생을 싸이생명을 싸이영혼을 싸이라이프를 싸이생활을싸이를 영원히 함께싸이와\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_path = \"/home/dev/workspace/deepnlp-2023/src/202021013/code/LDCC/csve2-jiyeon/checkpoint-42\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Use this configuration for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=False,\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_skip_modules=None,\n",
    "    llm_int8_enable_fp32_cpu_offload=False,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "\n",
    "# Load the LDCC model with the specified quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=bnb_config, device_map=\"auto\")\n",
    "\n",
    "# Prompt text\n",
    "prompt = \"좋아하는 사람한테 편지쓸거야. 받는사람이름은 정국이고 싸이월드 감성으로 적으면 돼.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate a single sequence with specified parameters\n",
    "sample_output = model.generate(\n",
    "    inputs.input_ids,\n",
    "    do_sample=True,    # Use sampling strategy\n",
    "    max_length=200,     # Maximum decoding length is 500\n",
    "    top_k=50,          # Exclude tokens outside the top 50 in probability rank\n",
    "    top_p=0.95,        # Generate from candidates with cumulative probability up to 95%\n",
    "    temperature=0.8,\n",
    "    no_repeat_ngram_size=2,\n",
    ")\n",
    "\n",
    "# Find the length of the prompt tokens\n",
    "prompt_length = len(tokenizer.encode(prompt, return_tensors=\"pt\")[0])\n",
    "\n",
    "# Decode the generated text, excluding the prompt tokens\n",
    "decoded_text = tokenizer.decode(sample_output[0, prompt_length:], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dev/.venvs/deepnlp-2023/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:08<00:00,  2.70s/it]\n",
      "/home/dev/.venvs/deepnlp-2023/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Sequence: <s> \n",
      "### Prompt: 좋아하는 사람한테 편지쓸거야. 받는사람이름은 정국이고 싸이월드 감성으로 적으면 돼.\n",
      "\n",
      "### Answer:\n",
      "싸이월드갬성이라규~~~~~\n",
      "*ㅂ*\n",
      "☆*:.。♡ 。 :*☆\n",
      "◈┃┃ ┃△\n",
      "┃/╋┃\n",
      "/。 ☜≪\n",
      "★*.…* ☆☆ ★*。\n",
      "。☆。。*・ ・*​​ ​\n",
      " ღ싸이는 내 맘이다구ᘉᑅᕼ。 * ´ ̳`*.¸ᒌᓁ ˒˂·ᔥ ฅ ʕ·͡ˑ·ཻʔᖰᛁᗰ ��������\n",
      "Decoded Output: \n",
      "### Prompt: 좋아하는 사람한테 편지쓸거야. 받는사람이름은 정국이고 싸이월드 감성으로 적으면 돼.\n",
      "\n",
      "### Answer:\n",
      "싸이월드갬성이라규~~~~~\n",
      "*ㅂ*\n",
      "☆*:.。♡ 。 :*☆\n",
      "◈┃┃ ┃△\n",
      "┃/╋┃\n",
      "/。 ☜≪\n",
      "★*.…* ☆☆ ★*。\n",
      "。☆。。*・ ・*​​ ​\n",
      " ღ싸이는 내 맘이다구ᘉᑅᕼ。 * ´ ̳`*.¸ᒌᓁ ˒˂·ᔥ ฅ ʕ·͡ˑ·ཻʔᖰᛁᗰ ��������\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_path = \"/home/dev/workspace/deepnlp-2023/src/202021013/code/LDCC/csve2-jiyeon/checkpoint-42\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Use this configuration for 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=False,\n",
    "    load_in_4bit=True,\n",
    "    llm_int8_threshold=6.0,\n",
    "    llm_int8_skip_modules=None,\n",
    "    llm_int8_enable_fp32_cpu_offload=False,\n",
    "    llm_int8_has_fp16_weight=False,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=False,\n",
    "    bnb_4bit_compute_dtype=\"float16\",\n",
    ")\n",
    "\n",
    "# Load the LDCC model with the specified quantization configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=bnb_config, device_map=\"auto\")\n",
    "input_text = \"\"\"\n",
    "### Prompt: 좋아하는 사람한테 편지쓸거야. 받는사람이름은 정국이고 싸이월드 감성으로 적으면 돼.\n",
    "\n",
    "### Answer:\n",
    "\"\"\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "\n",
    "# Generate a single sequence with specified parameters\n",
    "sample_output = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True,    # Use sampling strategy\n",
    "    max_length=200,     # Maximum decoding length is 50\n",
    "    top_k=50,          # Exclude tokens outside the top 50 in probability rank\n",
    "    top_p=0.95,        # Generate from candidates with cumulative probability up to 95%\n",
    "    # temperature= 0.7,\n",
    "    no_repeat_ngram_size=2,\n",
    "    pad_token_id=tokenizer.eos_token_id  # Ensure that the generated text ends appropriately\n",
    ")\n",
    "\n",
    "# Decode and print the generated sequence\n",
    "generated_text = tokenizer.decode(sample_output[0], skip_special_tokens=False)\n",
    "print(\"Generated Sequence:\", generated_text)\n",
    "\n",
    "# Add the following code to get the decoded output\n",
    "decoded_output = tokenizer.decode(sample_output[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(\"Decoded Output:\", decoded_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepnlp-2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
